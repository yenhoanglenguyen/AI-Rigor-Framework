# ðŸ”¬ Detailed Methodology: Applying Scientific Rigor to LLM Evaluation

## 1. Experimental Design Transfer
The evaluation framework is derived from principles used in structural biology and computational chemistry research (e.g., using **CCP4** for validation or **Igor Pro** for data analysis).

* **Controls:** Each prompt scenario includes a defined 'Control Prompt' (generic, low-context) against an 'Experimental Prompt' (high-context, pedagogically anchored).
* **Blinded Evaluation:** Outputs are stripped of metadata and evaluated by a diverse panel of teachers using a **blinded, rubric-based approach** to minimize evaluator bias.

## 2. Quantitative Metrics and Data Analysis
Evaluation results are captured in a structured format suitable for analysis in platforms like **MS PowerBI** or **Tableau**.

* **Key Metrics:**
    * **Fidelity Score:** Alignment with specific state standards (e.g., TEKS, NGSS).
    * **Utility Score:** Measure of immediate classroom applicability (e.g., lesson planning efficiency).
    * **Bias Index:** Statistical measure of output variation across demographic inputs.

* **Transferable Skills:** The analysis relies on **basic statistical knowledge** and the structured data manipulation skills developed using advanced platforms like **Igor Pro** and **command-based query logic** during prior research.

## 3. Tool Prototyping and Design
The framework is also used to guide the development and prototyping of custom GenAI tools.

* **Design Platform:** Initial design specifications for AI tool architecture (e.g., user flow, API integration logic) are documented using similar process control steps utilized in **AutoCAD** for microfluidic device design.
