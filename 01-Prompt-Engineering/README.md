# ðŸ’¡ 01. Prompt Engineering: Strategy and Standardization

This directory houses the **Prompt Library** and the standardized methodology for constructing high-fidelity, pedagogically aligned GenAI prompt chains. The goal is to move beyond simple commands to achieve predictable, measurable outputs that meet district-level quality standards.

## Key Sub-Sections:

### A. The Prompt Library
**Storage:** Contains the raw, successful prompt chains used for deployment (e.g., Lesson Planning, Assessment Generation, Feedback Tools).
* **Location:** [Link to 01-Prompt-Engineering-Library/ (Folder will be created in the next step)]

### B. Evaluation Data and Results
**Rigor:** Contains the systematic scores and validation metrics applied to prompt outputs to ensure quality, equity, and alignment to HQIM standards.
* **Location:** [See H-TESS Validation Scores Here](../02-Evaluation-Data/H-TESS-Evaluation-V1-Scores.md)

## ðŸš€ Standardization Principles

Every prompt chain adheres to these principles of **Scientific Rigor**:
1.  **Role Assignment:** Constraining the LLM to act as a certified expert (e.g., Texas AP Chemistry Expert).
2.  **Constraint Injection:** Forcing the LLM to integrate specific educational frameworks (e.g., 5E Model, UDL principles).
3.  **Iterative Refinement:** Using step-by-step prompts to correct initial deficiencies, treating each prompt as a variable in an experimental process.
